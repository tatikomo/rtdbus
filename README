Макет комплекса обмена сообщениями и доступа к БДРВ.

Используемые продукты и технологии
1. zeromq 4
2. eXtremeDB 3.5 & 5.0
3. google protobuf 2.5.1
4. google logging 1.7.0
5. xercesc 3.1.1
6. CodeSynthesis XSD XML Schema to C++ compiler 3.3.0

Архитектура передачи сообщений
--------------------------------------------------------------------------------
Между Клиентом и Брокером: DEALER (К) <=> ROUTER (Б)
Между Брокером и Обработчиком: ROUTER (Б) <=> DEALER (О)

Для прямого взаимодействия между Клиентом (К) и Обработчиком (О) нужно реализовать:
1. Каждый Сервис, помимо подключения к Брокеру (Б), создает сокет подписки SUB.
   Все (О) данного Сервиса имеют одинаковую подписку. Она передается в (Б) в сообщении
   READY
2. При получении данного сообщения (Б) заносит идентификатор подписки (строка типa
   "tcp://*.5555"?, может быть использовать в этом качестве само название Сервиса?)
3. Клиент, используя служебное сообщение (mmi.service?), запрашивает готовность и
   идентификатор подписки интересующего его Сервиса. Он создает два сокета - один для
   связи с (Б), второй - для прямого взаимодействия с (О). Проверить, возможно ли 
   использовать один сокет для (О), для связи с несколькими (О)
4. При необходимости, (К) может публиковать в канал подписки (О) нужного Сервиса
   свой запрос. Первый (О), получивший запрос, обрабатывает его и отправляет (К) на
   его приемный сокет типа DEALER. Проверить, может ли получать сокет данного типа
   одиночные сообщения).

Альтернатива: в (О) может быть несколько нитей для обслуживания запросов. Главный
процесс (О) получает запросы от (К) на сокет ROUTER, и передает из через inproc://workers
остальным своим нитям (смотри пример zguide/C/mtserver.c, asyncsrv). Обмен ROUTER<->ROUTER возможен,
но не прост (согласно ZMQ guide), т.к. необходимо заранее знать и указывать в передаваемом сообщении
IDENTITY адресата.

ROUTER will route messages asynchronously to any peer connected to it, if you prefix the identity as the first frame of the message.
Think of REQ and DEALER sockets as "clients" and REP and ROUTER sockets as "servers"

ОТЛАДКА
--------------------------------------------------------------------------------
Для корректной работы valgrind при обработке программ с включенной библиотекой 
ZMQ необходимо использовать следующую командную строку:
valgrind --tool=memcheck --leak-check=full --suppressions=valgrind.supp someprog

Перед компиляцией создать ссылки для zmq.h zmq.hpp из cppzmq/include в zeromq/src.

XML
--------------------------------------------------------------------------------
Валидация XML по примеру статьи http://stackoverflow.com/questions/124865/xml-schema-xsd-validation-tool
1) xmllint --noout --schema XSD_FILE XML_FILE
2) StdInParse -n -s -f -v=always < XML_FILE
Онлайн-инструменты по валидации и сравнению версий XML-документов http://www.corefiling.com/opensource

XSD версии 3.3.0 содержит ошибки, приводящие к множественным предупреждениям
компилятора gcc. Необходимо использовать флаг -fpermissive для успешной компиляции.
Версия 4.0 компилируется успешно без предупреждений компилятора.

Настройка отладочного вывода 
--------------------------------------------------------------------------------
Используется библиотека glog от GOOGLE, устанавливается в /usr/local
Использование GLOG
If the Google gflags library isn't installed, you set flags via environment variables, prefixing the flag name with "GLOG_", e.g.
   GLOG_logtostderr=1 ./your_application
The following flags are most commonly used:

logtostderr (bool, default=false)
Log messages to stderr instead of logfiles.
Note: you can set binary flags to true by specifying 1, true, or yes (case insensitive). Also, you can set binary flags to false by specifying 0, false, or no (again, case insensitive).
stderrthreshold (int, default=2, which is ERROR)
Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.
minloglevel (int, default=0, which is INFO)
Log messages at or above this level. Again, the numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.
log_dir (string, default="")
If specified, logfiles are written into this directory instead of the default logging directory.
v (int, default=0)
Show all VLOG(m) messages for m less or equal the value of this flag. Overridable by --vmodule. See the section about verbose logging for more detail.
vmodule (string, default="")
Per-module verbose level. The argument has to contain a comma-separated list of <module name>=<log level>. <module name> is a glob pattern (e.g., gfs* for all modules whose name starts with "gfs"), matched against the filename base (that is, name ignoring .cc/.h./-inl.h). <log level> overrides any value given by --v. See also the section about verbose logging.
There are some other flags defined in logging.cc. Please grep the source code for "DEFINE_" to see a complete list of all flags.

Используется ctags (exuberant-ctags) для создания библиотеки перекрестных ссылок в исходных текстах
Файл tags создается командой:
ctags -R --c++-kinds=+p --fields=+iaS --extra=+q .

Используется cppcheck для статической проверки исходных текстов:
cppcheck . --force -I../libzmq/src -I../eXtremeDB_5_fusion_eval/include -I../glog/src -I../protobuf/src -I../googletest/googletest/include -Ihdb -Iexchange/config -Iexchange/sa -Iexchange/smad -Iexchange/egsa -Iclient -Iworker -Itest -Imsg -Itools -Ixdb/impl -Ixdb/rtap -Ixdb/broker -Icmake -Imdp

cppclean . -I../libzmq/src -I../eXtremeDB_5_fusion_eval/include -I../glog/src -I../protobuf/src -I../googletest/googletest/include -I../rapidjson/include -I../sqlite/sqlite-autoconf-3150100 -I../cppzmq -Ihdb -Iexchange/config -Iexchange/sa -Iexchange/smad -Iexchange/egsa -Iclient -Iworker -Itest -Imsg -Itools -Ixdb/impl -Ixdb/common -Ixdb/rtap -Ixdb/broker -Icmake -Imdp -I../xerces-c/src -I../libmodbus/src -I. -I../samples/zguide/examples/C++

Подробная трассировка тестов включается с помощью:
ctest -R НАЗВАНИЕ_ТЕСТА -VV

Хронология проблем:
--------------------------------------------------------------------------------
04.03.2013 = Решено
Worker не получает сообщения от Брокера. После нескольких безответных (с точки зрения worker-а) попыток передачи HEARTBEAT сервис перезапускается. Эта ситуация диагностируется Брокером успешно.

13.03.2013 
Проблема от 04.03.2013 решена. Исправлена ошибка в поле сообщения, используемая протоколом ZMQ в качестве адресата.

14.03.2013
Бинарная совместимость с протоколом Majordomo 0.2.0 

15.03.2013
Опробована работа под Solaris 9:
- всей связки целиком
- брокер на linux, служба в solaris
Обнаружено постепенно увеличивающееся с каждой итерацией количество сообщений HEARTBEAT, 
посылаемых службе единовременно. 

27.08.2013
У библиотеки ZMQ (как минимум в 3.2) есть потенциальная проблема с сокетами ROUTER:
если удаленная сторона не отвечает на запрос, происходит утечка памяти. 
An unavoidable memory leak in ROUTER sockets when there is pending data for clients who will never return.
Ссылка на статью: http://gonitro.io, https://github.com/bumptech/nitro

11.09.2013 = Решено
Чтение содержимого сериализованных полей Сообщения из базы данных завершается без ошибок, но данные не соответствуют исходным. Была проблема в некорректном указании длины строки, читаемой из БД, для последующей десериализации.

13.09.2013 = Решено
Нарушение уникальности ключа PK_expire_for_service у Letter приводило к невозможности иметь более одного экземпляра
сообщения в БД с одинаковыми секундными отметками времени. Однако у одной Службы может быть несколько одновременно
поступивших изменений, пришлось убрать тег unique с этого индекса.

05.02.2015
В некоторых случаях Клиент, начиная новый(ые) запрос(ы) после паузы, не получает ответа от Службы.
В этот период (2.5сек) Служба не может ответить, поскольку находится внутри вызова sleep(HeartbeatInterval)
после обнаружения необходимости переподключения к Брокеру.
Проблема в том, что после завершения обработки Службой последнего запроса Клиента прошло время,
превышающее таймаут, а Служба в этот период не посылала HEARTBEAT Брокеру. Нужно следить за периодичностью
запросов HEARTBEAT в адрес Брокера на стороне Службы.

25.05.2015 = Решено
Если Клиент попытался отправить сообщение несуществующуему Сервису, а затем завершает свою работу,
то он "зависает" в деструкторе mdcli на удалении контекста в mdp_client_async_api.cpp, "delete m_context".
Включены таймауты на получение и отправку данных конфигурируемой длительности (config.h).

02.07.2015 = Решено
Иногда после нештатного завершения процесса-Клиента Диггер не может завершить свою работу штатно.
Прерывание возможно только отправкой сигнала SIGKILL.
При этом в логе наблюдается, что все нити DiggerWorker получили команду TERMINATE, DiggerProxy пишет о том,
что все нити DiggerWorker в состоянии joinable, однако в контексте DiggerProxy команда join завершается только 
для двух нитей из трех.
Пример из лога:
I0703 18:00:07.242611  9824 mdp_worker_api.cpp:60] Got signal 2
E0703 18:00:07.242763  9824 mdp_worker_api.cpp:378] Interrupted system call
W0703 18:00:07.243075  9824 mdp_worker_api.cpp:388] Interrupt received, killing worker...
I0703 18:00:07.243093  9824 wdigger.cpp:358] Digger::recv() got a NULL
I0703 18:00:07.243103  9824 wdigger.cpp:471] Send TERMINATE to DiggerProxy
I0703 18:00:07.243141  9824 wdigger.cpp:392] Waiting joinable DiggerProxy thread
I0703 18:00:07.243299  9827 wdigger.cpp:214] DiggerProxy zmq::proxy finish successfuly
I0703 18:00:07.243320  9827 wdigger.cpp:225] Send TERMINATE to DiggerWorker 0
I0703 18:00:07.243345  9827 wdigger.cpp:225] Send TERMINATE to DiggerWorker 1
I0703 18:00:07.243358  9827 wdigger.cpp:225] Send TERMINATE to DiggerWorker 2
I0703 18:00:07.243383  9827 wdigger.cpp:233] DiggerProxy cleanup 3 workers and threads
I0703 18:00:07.243397  9827 wdigger.cpp:237] delete DiggerWorker[1/3] 0xb47009a0, thread 0xb4709858, joinable: 1
I0703 18:00:07.243402  9829 wdigger.cpp:86] received request:
I0703 18:00:07.243417  9828 wdigger.cpp:86] received request:
I0703 18:00:07.243404  9830 wdigger.cpp:86] received request:
I0703 18:00:07.243454  9828 mdp_zmsg.cpp:369] --------------------------------------
I0703 18:00:07.243463  9829 mdp_zmsg.cpp:369] --------------------------------------
I0703 18:00:07.243470  9828 mdp_zmsg.cpp:397] [009] TERMINATE
I0703 18:00:07.243479  9830 mdp_zmsg.cpp:369] --------------------------------------
I0703 18:00:07.243489  9828 wdigger.cpp:93] Got TERMINATE command, shuttingdown this DiggerWorker thread
I0703 18:00:07.243501  9830 mdp_zmsg.cpp:397] [009] TERMINATE
I0703 18:00:07.243505  9828 xdb_rtap_connection.cpp:29] Destroy RtConnection for env SINF
I0703 18:00:07.243512  9830 wdigger.cpp:93] Got TERMINATE command, shuttingdown this DiggerWorker thread
I0703 18:00:07.243517  9828 xdb_impl_connection.cpp:44] Closing database 'SINF' connection 0xb5101c08
I0703 18:00:07.243525  9830 xdb_rtap_connection.cpp:29] Destroy RtConnection for env SINF
I0703 18:00:07.243479  9829 mdp_zmsg.cpp:397] [009] TERMINATE
I0703 18:00:07.243533  9830 xdb_impl_connection.cpp:44] Closing database 'SINF' connection 0xb5101c08
I0703 18:00:07.243544  9829 wdigger.cpp:93] Got TERMINATE command, shuttingdown this DiggerWorker thread
I0703 18:00:07.243558  9829 xdb_rtap_connection.cpp:29] Destroy RtConnection for env SINF
I0703 18:00:07.243566  9829 xdb_impl_connection.cpp:44] Closing database 'SINF' connection 0xb5101e8c
I0703 18:00:07.243623  9828 wdigger.cpp:144] DiggerWorker thread is done
I0703 18:00:07.243680  9827 wdigger.cpp:242] DiggerProxy joins DiggerWorker' tread 1
I0703 18:00:07.243696  9829 wdigger.cpp:144] DiggerWorker thread is done
I0703 18:00:07.243700  9827 wdigger.cpp:53] DiggerWorker destroyed
I0703 18:00:07.243732  9827 wdigger.cpp:237] delete DiggerWorker[2/3] 0xb4709948, thread 0xb4709848, joinable: 1
I0703 18:00:07.243751  9827 wdigger.cpp:242] DiggerProxy joins DiggerWorker' tread 2
I0703 18:00:07.243763  9827 wdigger.cpp:53] DiggerWorker destroyed
I0703 18:00:07.243777  9827 wdigger.cpp:237] delete DiggerWorker[3/3] 0xb470a278, thread 0xb470a188, joinable: 1
I0703 18:01:18.845397  9824 mdp_worker_api.cpp:60] Got signal 15
<kill -9 9824>

10.11.2015
При чтении списка точек, входящих в подписку, среди нормальных точек появляется точка с нулевым autoid_t.
Смотри файл xdb_impl_db_rtap.cpp:1941.
При этом в лог выводится ошибка о невозможности чтения данных точки с таким идентификатором, поскольку
идентификаторы в БДРВ начинаются с 1.

TODO
--------------------------------------------------------------------------------
Требование:
Проверить возможность использования std::string в качестве полезной нагрузки.
Можно ли хранить protobuf в std::string? Стоит перейти на zframe вместо string?
Ответ:
Тип данных string пригоден для хранения UTF-8 и бинарных данных. Локализованные строки нормально хранятся и
читаются из Базы данных.

Требование:
Рассмотреть возможность внедрения поддержки UNICODE, используя библиотеку ICU.

Версия 3.5 eXtremeDB не поддерживает неуникальные таблицы хешей. Множество одинаковых
значений могут содержать только деревья. Хеши версии 4.1 и выше могут содержать
неуникальные ключи. Так как текущая версия БД равна 3.5, но впоследствии планируется
переход на 4.1, можно использовать хеши. Это повлечет ограничение - у каждого Сервиса
может быть только один Обработчик. После перехода на 4.1 это ограничение можно снять.
Или в 3.5 использовать деревья, а после миграции на 4.1 перейти на хеши, меньше
накладных расходов.

Требование:
PushWorker() для существующего Обработчика обновит его поле состояния, оставив все остальное 
без изменений. Возможно требуется очистка полей этого Экземпляра - удаление связанных 
Сообщений и т.п.

Требование:
Значения интервалов HEARTBEAT у Брокера и Обработчиков задаются в разных местах. Нужно объединить.
Решение:
Период HEARTBEAT задается в config.h.

Требование:
Проверить условия получения идентификатора отправителя "mmi.*", это приводит к удалению Обработчика.
Решение:
Это название Службы, к которой адресовано служебное сообщение.

Требование:
Рассмотреть необходимость присутствия в заголовке строковых полей "Отправитель" и "Адресат". Эта информация
дублирует систему идентификаторов (IDENTITY), и увеличивает размер заголовка.
Решение:
Оставить отдельные поля "Отправитель" и "Адресат", поскольку они относятся к уровню приложения,
а IDENTITY относится к транспортному уровню, их значения несут разный смысл.

Выбирать из очереди сообщений для передачи их свободному Обработчику следует те экземпляры, 
которые ждут своей очереди более остальных.

Проверить, верно ли что: возврат из mdcli->recv() с нулевым сообщением означает получение ответа 
на сообщение, просроченного более чем на 2 секунды? К примеру, запрос был послан 2.5 секунды назад, 
и только что получен ответ, хотя сокет DEALER его уже не ждал?

Требование: Минимизировать количество ходящих сообщений HEARTBEAT. Если от процесса было сообщение не далее чем 
величина интервала HEARTBEAT, не посылать ему этот запрос. В файле mdp_broker.cpp, функция worker_waiting(), 
строка 685, не вызывается worker_send(HEARTBEAT...) согласно zguide/C/mdbroker.c
Решение:
Не посылать HEARTBEAT-сообщение для процессов, интенсивно обменивающихся с Брокером. Коммит b4b92b3.

Требование:
Ввести в Брокере фоновую нить, проверяющую наличие устаревших Обработчиков и Сообщений.
Если Обработчик устарел, и не имеет связанного сообщения, удалить его из БД.
Если Сообщение устарело, передать его на исполнение другому обработчику или ответить Клиенту отказом.

Требование:
Убрать общие детали реализации MCO внутрь класса-имплементации DatabaseImpl для DatabaseBrokerImpl.
Сейчас внутри DatabaseBrokerImpl часть функций дубируется в для DatabaseImpl, причем перевес в реализации
на стороне DatabaseBrokerImpl. Из возможностей DatabaseImpl используется только получение имени БД и функции
смены внутренних состояний.
Реализовано.

Требование:
Обеспечить для Клиента хранение в постоянном хранилище запросов, поданных Брокеру с его стороны.
После аварийного перезапуска Клиента обеспечить повторную отправку этих запросов Брокеру. Клиенту 
при получении ответа на запрос нужно удалять его запись из постоянного хранилища.
Предложение - при аварийном перезапуске Клиент уведомляет об этом Брокера, и тот сам повторно
передает запросы Клиента Службам (что при этом произойдет с идентификатором Клиента?)

Требование:
Автоматически создавать группу подписки EGSA при создании БДРВ из текстового представления.
В группу должны входить все точки типа "Система Сбора". Это необходимо для оперативного оповещения
модуля EGSA об изменении состояния подчиненных систем сбора.

Требование:
2017/05/17
Хранить данные об активных Циклах, Сайтах и связанных с ними Запросов в БД XDB.
Это необходимо в связи с тем, что экземпляры Request, AcqSiteEntry и Cycle используются и могут менять
свое состояние в трех разных нитях - EGSA::implementation, EGSA::implementation_acq и
EGSA::implementation_send, в связи с чем требуется обеспечить синхронизацию.

Удаленные сетевые и локальные репозитории
На gev-book добавлен сетевой репозиторий gev@gev-itg:/var/git/rtdbus.git
На gev-itg добавлен локальный промежуточный репозиторий integration
  gev@gev-itg:/var/git/rtdbus.git$ git remote -v
    origin  /home/gev/ITG/sandbox/rtdbus (fetch)
    origin  /home/gev/ITG/sandbox/rtdbus (push)
  gev@gev-book ~/ITG/rtdbus $ git remote -v
    origin  gev@gev-itg:/var/git/rtdbus.git (fetch)
    origin  gev@gev-itg:/var/git/rtdbus.git (push)
  gev@gev-itg:~/ITG/sandbox/rtdbus$ git remote -v
    integration   /var/git/rtdbus.git (fetch)
    integration   /var/git/rtdbus.git (push)
    origin    git@bitbucket.org:gev76/rtdbus.git (fetch)
    origin    git@bitbucket.org:gev76/rtdbus.git (push)

Требование:
Предусмотреть отдельную нить мониторинга соединений с БДРВ для Службы БДРВ.
Примеры реализации см. в "samples/core/19-recovery/sniffer/"

Требование:
Реализовать службу/службы генерации, сохранения, и выдачи по заявке исторических значений.
Реализация:
Сохранению подлежат два значения атрибутов VAL и VALID. Были проверены четыре механизма хранения
исторических данных.
1) В виде файлов с названием тега параметра, внутри дерева каталогов из родительских точек;
К примеру, параметр /KD4005/GOV20/PT01 хранил множество своих значений для определенной дискретизации
в файле KD4005/GOV20/PT01.<тип дискретизации>
где тип дискретизации - числовой идентификатор от 0 (одноминутные семплы) до 5 (ежемесячные семплы).
2) В БД REDIS;
Использовались команды ZADD, ZRANGE WITH SCORE, где в качестве SCORE выступало числовое значение отметки
времени в виде 64-битного целого числа секунд. Все отметки времени для соответствующих интервалов
были выравнены по границам этих интервалов. Это позволяло выстро находить временнЫе интервалы, однако
после этого приходилось внутри найденного интервала искать нужный тег. Помимо этого, размер базы Истории
был ограничен размером ОЗУ, и после начала накопления требования к объему ОЗУ постоянно увеличивались,
что в реальной эксплуатации затруднило бы совместное использование БДРВ и БД Истории на одном устройстве.
3) В БДРВ, в таблицах HISTORY;
Оказалось затруднительно совместить transient- и persistent-таблицы в рамках одной базы XDB, поскольку
документация не содержит сведений о способе одновременной активации флагов mco_shm_supported и
mco_disk_supported. Помимо этого, размер снимка базы XDB значительно увеличивается при накоплении более
тысячи записей в HISTORY, с 10Мб до 30Мб. Принято решение оставить в БДРВ таблицы, размер которых не
меняется значительно в процессе работы. Это позволит снизить системные требования (объем ОЗУ) к серверам.
4) В снимках SQLite;
Выглядит наиболее предпочтительно. Низкие требования к ОЗУ, возможен одновременный доступ к таблицам из
разных нитей/процессов, поддерживается ACID. Из недостатков - невысокая производительность.

Требование:
Обеспечить простой и прозрачный механизм выгрузки информации по Точкам заданного свойства.
Реализация:
Проверить, стоит ли перенести понятие Категории в БДРВ, для создания единого запроса по
выгрузки информации о Точках нужной категории? Маска категории может формироваться
на этапе генерации БДРВ, а может быть синтетической. В первом случае генератор БДРВ
формирует маску Категории, основываясь на свойстве генерируемой сейчас Точки.
В последнем случае информация о категории нигде в явном не хранится, но при запросе
выборки соответствующих данных поисковый движок будет учитывать характеристики Точки.
К примеру, запрос на получение списка Точек с плав. точкой, имеющих предысторию:
(1) rtQUERY_PTS_IN_CATEG(CATEG_ANALOG | CATEG_HISTORY_EXISTS)
или
(2) rtQUERY_PTS_IN_CATEG(CATEG_ANALOG_HISTORY_EXISTS)

Требование:
2016/05 Обеспечить Обработчики возможностью обмениваться сообщениями друг с другом.
Решение:
В интерфейс mdwrk добавить методы отправки сообщения к указанному сервису. Если Сервис-адресат
известен, отправить ему прямое сообщение. Если неизвестен, то сначала у Брокера получить для него
строку подключения, и запомнить ее для последующего использования при повторной отправке.

Требование:
2017/06 Проверить возможность замены LED в словарях Обмена ESG на некую контрольную сумму от TAG.
Это позволит отказаться от поддержания синхронности словарей на двух сторонах информационного обмена,
поскольку при получении запроса с определенным LED принимающая сторона просто найдет нужный TAG в SMED,
один раз получив контрольную сумму для всех своих параметров.

==========================================================
(gdb) print *((mco_db_connection_t*)(con0))
{
  conn_id = 0,
  tran_seq = 77,
  status = 0,
  curr_obj_class = 2,
  curr_obj = 0, 
  dm = 
  {
    hdr = 0x0,
    pages = 0x0,
    file = 0x0,
    log = 0x0, 
    logs = {0x0, 0x0},
    dirty_pages = 0x0,
    write_delayed_pages = 0x0,
    root_page = 0x0,
    btree_buf = 0x0,
    log_write_buf = 0x0,
    file_write_buf = 0x0,
    dbh = 0x0,
    con = 0x0,
    hash_table = 0x0, 
    page_data = 
    {
      0x0 <repeats 16 times>
    },
    updating = 0,
    mutex = 0x0,
    alloc_mutex = 0x0,
    policy = MCO_COMMIT_SYNC_FLUSH,
    n_committed_transactions = 0,
    truncate_log = 0,
    in_critical_section = 0,
    cache_enabled = 0,
    sems = 0x0,
    rw_locks = 0x0,
    alloc_callback = 
    {
      func = 0x0,
      threshold_kind = MCO_ALLOC_PERCENT_USED,
      threshold_value = 0,
      prev_value = 0
    },
    n_modified_pages = 0,
    n_precommitted_pages = 0,
    rc4_state = 0x0,
    page_cache = 
    {
      {timestamp = 0, access_count = 0, index = 0},
      {timestamp = 0, access_count = 0, index = 0},
      {timestamp = 0, access_count = 0, index = 0},
      {timestamp = 0, access_count = 0, index = 0}
    },
    curr_timestamp = 0
  }, 
  trans = 
  {
    active_trans_list = { next = 0, prev = 0}, 
    error_code = MCO_S_OK, 
    trans_no = 54, 
    snapshot_no = 53, 
    n_performed_transactions = 76, 
    latest_trans_no = 0, 
    n_level = 0, 
    flags = 0, 
    prio = 0, 
    isolation_level = MCO_REPEATABLE_READ, 
    type = MCO_READ_WRITE, 
    tm = 0xb5d257ec, 
    next = 0, 
    wait = 
    {
      flags = 0 '\000', 
      db_index = 0 '\000', 
      sync_index = 0
    },
    cleanup_period = 1, 
    private_vlist = 
    {
      head = 0,
      tail = 0,
      free = 0,
      first_pos = 0,
      curr_pos = 0,
      max_elems = 0,
      max_index = 0,
      size = 0
    },
    curr_vlist = 0xb5d2922c, 
    param = 0x0, 
    ha_iterator = 0x0, 
    commit_iterator = 0x0, 
    last_inspected_trans_no = 0
  }, 
  db = 0xb5d22144, 
  sync_data = 0xb5d257ac, 
  pmh = 0xb5d39780, 
  free_pages = 0xb5daef80, 
  n_free_pages = 220, 
  default_isolation_level = MCO_DEFAULT_ISOLATION_LEVEL, 
  index_table = 0xb5d2960c, 
  pm_inmem = 0xb7f70fa0 <inmemory_pm>,
  pset = 0xb5d2963c, 
  udfs = 0x0, 
  colls = 0x0, 
  iter_index = 0x0, 
  hnd_cache = 
  {
    {obj = 0, hnd = 0x0},
    {obj = 0, hnd = 0x0},
    {obj = 0, hnd = 0x0},
    {obj = 524364, hnd = 0xbfffeda8},
    {obj = 0, hnd = 0x0},
    {obj = 0, hnd = 0x0},
    {obj = 0, hnd = 0x0},
    {obj = 0, hnd = 0x0}
  },
  hnd_cache_index = 796,
  process_id = 1,
  wc = 
  {
    root = 0x0,
    head = 0x0,
    tail = 0x0,
    height = 0,
    size = 0,
    threshold = 0,
    n_entries = 63,
    n_children = 255,
    curr = 0
  },
  ctx = 0x0,
  leaf_page = 0x0,
  leaf_page_access_count = 0 '\000',
  leaf_page_access_kind = 1 '\001',
  btree_access_kind = 2 '\002',
  inside_delete_all = 0 '\000',
  voluntary_index = 0,
  check_constraints = 1,
  alloc_callback = 
  {
    func = 0x0,
    threshold_kind = MCO_ALLOC_PERCENT_USED,
    threshold_value = 0,
    prev_value = 0
  }, 
  iter_pages = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
  current_rcursor = 0,
  last_autoid = 24,
  emark = 21140
}

0000000 0000 0000 004d 0000 0000 0002 0000 0000
0000010 0000 0000 0000 0000 0000 0000 0000 0000
*
0000110 0036 0000 0000 0000 0035 0000 0000 0000
0000120 004c 0000 0000 0000 0000 0000 0000 0000
0000130 0000 0000 0000 0000 0000 0000 0002 0000
0000140 0002 0000 57ec b5d2 0000 0000 0000 0000
0000150 0001 0000 0000 0000 0000 0000 0000 0000
0000160 0000 0000 0000 0000 0000 0000 0000 0000
0000170 0000 0000 922c b5d2 0000 0000 0000 0000
0000180 0000 0000 0000 0000 0000 0000 2144 b5d2
0000190 57ac b5d2 9780 b5d3 ef80 b5da 00dc 0000
00001a0 0000 0000 960c b5d2 0fa0 b7f7 963c b5d2
00001b0 0000 0000 0000 0000 0000 0000 0000 0000
*
00001d0 0000 0000 004c 0008 eda8 bfff 0000 0000
00001e0 0000 0000 0000 0000 0000 0000 0000 0000
00001f0 0000 0000 0000 0000 0000 0000 031c 0000
0000200 0001 0000 0000 0000 0000 0000 0000 0000
0000210 0000 0000 0000 0000 0000 0000 003f 0000
0000220 00ff 0000 0000 0000 0000 0000 0000 0000
0000230 0100 0002 0000 0000 0001 0000 0000 0000
0000240 0000 0000 0000 0000 0000 0000 0000 0000
*
0000270 0000 0000 0000 0000 0018 0000 0000 0000
0000280 5294 0000                              
0000284


